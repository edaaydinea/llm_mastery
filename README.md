# LLM Mastery: ChatGPT, Gemini, Claude, Llama3, OpenAI & APIs

Learn to master the latest in Large Language Models (LLMs) including ChatGPT, Gemini, Claude, Llama3, and OpenAI APIs. This course provides comprehensive insights and practical applications.

[Course Link](https://www.udemy.com/course/llm-mastery-chatgpt-gemini-claude-llama3-openai-apis)

## Table of Contents
- [LLM Mastery: ChatGPT, Gemini, Claude, Llama3, OpenAI \& APIs](#llm-mastery-chatgpt-gemini-claude-llama3-openai--apis)
  - [Table of Contents](#table-of-contents)
  - [Day 1: How LLMs Work: Parameters, Weights, Inference, Neural Networks and More](#day-1-how-llms-work-parameters-weights-inference-neural-networks-and-more)
  - [Day 2: Additional Capabilities of LLMs \& Future Developments](#day-2-additional-capabilities-of-llms--future-developments)
  - [Day 3: Prompt Engineering: Effective Use of LLMs in the Standard Interface](#day-3-prompt-engineering-effective-use-of-llms-in-the-standard-interface)
  - [Day 4: LLM Customization: System Prompts, Memory, RAG \& Creating Expert Models or GPTs](#day-4-llm-customization-system-prompts-memory-rag--creating-expert-models-or-gpts)
  - [Day 5: Closed-Sourse LLMs: An Overview of Available Models and how to use them](#day-5-closed-sourse-llms-an-overview-of-available-models-and-how-to-use-them)

## Day 1: How LLMs Work: Parameters, Weights, Inference, Neural Networks and More

**What I did today:**

- Reviewed the basics of Language Models (LMs) including pre-training and fine-tuning.
- Learned about the transformer architecture and reinforcement learning.
- Explored the components of an LM: parameter file and run file.
- Understood the process of creating parameter files using GPUs and text data.
- Studied neural networks and their functioning with forward and back propagation.
- Examined how neural networks work with word tokens in LLMs.
- Discussed the transformer architecture and its current limitations.
- Learned about the mixture of experts approach for transformer architecture.
- Reviewed the fine-tuning process to create assistant models.
- Understood reinforcement learning and its application in LLMs.
- Discussed the scaling laws of LLMs and the importance of GPU and data.

**Resources**:

- [day1 notes.ipynb](./notes/day1.ipynb)

## Day 2: Additional Capabilities of LLMs & Future Developments

**What I did today:**

- Explored the multimodal capabilities of LLMs, including processing images, audio, and video.
- Learned about ChatGPT's ability to use various tools like calculators and Python libraries.
- Studied the vision capabilities of LLMs, including image recognition and visual processing.
- Discussed the potential for LLMs to engage in natural, conversational speech.
- Reviewed the concept of System 1 and System 2 thinking in LLMs.
- Learned about recent updates to ChatGPT, including real-time web searches and content creation tools.
- Examined the advancements in OpenAI's O3 model towards AGI.
- Discussed the concept of self-improvement in AI inspired by AlphaGo.
- Explored methods for improving LLM performance, such as RAG and prompt engineering.
- Envisioned the future of LLMs as comprehensive operating systems.

**Resources**:

- [day2 notes.ipynb](./notes/day2.ipynb)

## Day 3: Prompt Engineering: Effective Use of LLMs in the Standard Interface


**What I did today:** 

- Gained an understanding of the fundamental principles of prompt engineering, applicable across various Large Language Models (LLMs).
- Identified common LLM interfaces such as ChatGPT, Hugging Chat, Copilot, and Gemini, noting their shared basic functionalities.
- Learned about token limits in LLMs and their importance in managing context within conversations.
- Recognized how different prompt phrasings can significantly impact an LLM's response, underscoring the importance of effective prompt engineering.
- Grasped the concept of semantic association and how words trigger related concepts in LLMs, influencing their responses.
- Explored the use of structured prompts, consisting of modifiers and topics, to guide LLMs towards more accurate and tailored outputs.
- Discovered the effectiveness of instruction prompting and the use of specific phrases like "Let's think step by step" to enhance LLM performance.
- Understood how role prompting can leverage semantic association by assigning specific roles to LLMs, resulting in more relevant responses.
- Learned about zero-shot, one-shot, and few-shot prompting techniques, which utilize examples to guide LLM outputs.
- Investigated reverse prompt engineering as a method to extract underlying prompts from text to replicate desired styles and content.
- Explored chain of thought prompting, which involves providing step-by-step reasoning to improve the accuracy of LLM responses, especially for complex tasks.
- Became familiar with the Tree of Thought prompting technique, a more advanced method for complex problem-solving involving the generation and evaluation of multiple solution paths.
- Reviewed the diverse applications of LLMs across various domains, emphasizing the importance of effective prompting to maximize their potential.
- Learned how to combine different prompting techniques, such as role prompting, structured prompts, and few-shot prompting, to achieve optimal LLM outputs, using a real-world example.
- Recapped the various prompt engineering techniques covered, including token limits, semantic association, and different prompting strategies, highlighting the importance of practical application.

**Resources**:

- [day3 notes.ipynb](./notes/day3.ipynb)

## Day 4: LLM Customization: System Prompts, Memory, RAG & Creating Expert Models or GPTs

**what I did today:**

- Explored fundamental methods for chatbot personalization, with a focus on the user-friendly interface of ChatGPT.
- Investigated the customization of ChatGPT's memory feature to retain user preferences and context across conversations.
- Examined the use of custom instructions as persistent system prompts for tailoring ChatGPT's responses based on personal context and desired behavior.
- Understood the concept of in-context learning as a short-term memory solution for language models.
- Learned about Sparse Prime Representation (SPR) as a more token-efficient approach to in-context learning.
- Gained knowledge of Retrieval-Augmented Generation (RAG) technology for enabling long-term memory in language models through vector databases.
- Discovered how to implement simplified RAG using custom GPTs within ChatGPT by uploading external knowledge files.
- Navigated the ChatGPT GPT Store, exploring various custom GPTs for code generation, PDF analysis, and YouTube video summarization.
- Identified three potential strategies for monetizing custom GPTs: revenue sharing, lead generation, and facilitating upsells.
- Understood the necessity of creating a builder profile in ChatGPT to potentially monetize GPTs and generate leads.
- Learned the process of creating a custom GPT with specific knowledge to generate leads and facilitate upsells by strategically incorporating links.
- Defined Application Programming Interfaces (APIs) as software components enabling communication between different applications.
- Explored the integration of Zapier actions into GPTs to automate tasks across various platforms like Gmail and Google Docs.
- Demonstrated the process of integrating external APIs into custom GPTs using the Cat API as a practical example.
- Synthesized the key learnings from the section, emphasizing the application of customization techniques for personalizing ChatGPT and leveraging GPTs for business purposes, particularly lead generation.

**Resources**:

- [day4 notes.ipynb](./notes/day4.ipynb)

## Day 5: Closed-Sourse LLMs: An Overview of Available Models and how to use them

**What I did today:**

- Analyzed video transcripts summarizing key aspects of various closed-source Large Language Models (LLMs) and related platforms.
- Identified the core functionalities, advantages, and disadvantages of models such as OpenAI's ChatGPT, Google's Gemini, and Anthropic's Claude.
- Summarized the features and use cases of platforms like Perplexity and Poe, which build upon these underlying LLMs.
- Understood the integration of OpenAI's technology into Microsoft's Copilot across various Microsoft 365 applications.
- Noted the specific capabilities of Copilot within Word, PowerPoint, Outlook, and Excel, as well as the introduction of Copilot GPTs.
- Reviewed GitHub Copilot as an AI-powered tool for programmers and its potential impact on coding efficiency.
- Synthesized the overall value proposition of Microsoft Copilot and its associated subscription models, comparing it to free alternatives.
- Compiled a summary of the key learnings regarding the discussed closed-source LLMs and their respective platforms.

**Resources**:

- [day5 notes.ipynb](./notes/day5.ipynb)