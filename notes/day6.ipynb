{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97a9d96",
   "metadata": {},
   "source": [
    "# What Is This About? APIs of Closed-Source LLMs\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section provides an introductory overview of working with closed-source APIs, specifically focusing on the OpenAI API to teach the basic concepts, playground usage, and billing before moving on to building applications.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ¤– Introduces closed-source APIs and the necessary steps to start using them.\n",
    "- ğŸ’¡ Focuses on the OpenAI API for initial exploration and understanding of basic concepts.\n",
    "- ğŸ® Emphasizes using the API playground to experiment and learn without building applications yet.\n",
    "- ğŸ’° Covers practical aspects like setting up a billing account for API usage.\n",
    "- ğŸ¤” Explains the fundamental concept of an API call to an Language Model (LLM) and receiving a response.\n",
    "- ğŸ“Š Aims to provide a broad overview of different APIs available, including cost considerations.\n",
    "- â–¶ï¸ Prepares the user for the next video which will delve into a quick overview of the OpenAI API.\n",
    "\n",
    "# Overview of the OpenAI API\n",
    "\n",
    "### Summary\n",
    "\n",
    "This segment clarifies that using powerful AI models doesn't require subscriptions but can be done via APIs like OpenAI's, where payment is based on token usage, and provides a detailed walkthrough of the OpenAI API documentation covering models, features, getting started, and moderation.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ’¸ You can access the best models via API without a subscription, paying only per token generated.\n",
    "- ğŸ The API can be integrated into environments like Google Colab or custom apps, commonly using Python, Curl, or Node.js.\n",
    "- ğŸ“„ The OpenAI API documentation details various models available, including GPT-4o, GPT-4 Turbo, and GPT-3.5.\n",
    "- âœ¨ The API offers diverse capabilities like text generation, embeddings, image generation (DALL-E), text-to-speech, speech-to-text (Whisper), fine-tuning, vision, and the Assistants API with function calling and code interpreter.\n",
    "- âš™ï¸ Key sections in the documentation include introduction, key concepts, quickstart guide (account setup, API key creation), tutorials, and moderation policies.\n",
    "- ğŸ› ï¸ The documentation includes resources like a help center, developer forum, and service status.\n",
    "- ğŸ’° API usage is described as relatively cheap, with a more in-depth discussion on pricing and API comparisons planned for the next video.\n",
    "\n",
    "# Pricing Models of the OpenAI API\n",
    "\n",
    "### Summary\n",
    "\n",
    "This section details the pricing of the OpenAI API, explaining the pay-per-token model for various models and capabilities, demonstrating its relative affordability compared to subscriptions, and outlining the costs for different services like text generation, embeddings, image generation, audio models, and the Assistant API.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ’² OpenAI API pricing is based on tokens (input and output), with options to view costs per 1,000 or 1 million tokens.\n",
    "- ğŸ“ˆ Pricing varies by model, with newer models like GPT-4o having different input and output token costs ($5 and $15 per 1 million tokens, respectively), while GPT-3.5 Turbo is significantly cheaper ($0.50 and $1.50 per 1 million tokens).\n",
    "- ğŸ“ 1 million tokens are approximately equivalent to 750,000 words, illustrating that even small amounts of text processing are very inexpensive.\n",
    "- ğŸ–¼ï¸ Costs for other API capabilities are also outlined, including embedding models (cheap), fine-tuning (training, input, output costs), Assistant API features (Code Interpreter session cost, file storage), and image generation with DALL-E (varying prices based on resolution and model).\n",
    "- ğŸ§ Audio models like Whisper (transcription) are presented as very cheap, while Text-to-Speech has a higher cost per character.\n",
    "- âœ… Overall API usage is described as generally cheap, especially for experimentation or simpler applications, potentially saving money compared to a ChatGPT subscription.\n",
    "- â–¶ï¸ The next video will demonstrate the OpenAI Playground, a fixed interface for experimenting with the API, where users only pay for tokens generated.\n",
    "\n",
    "# Important: OpenAI Playground overview and Billing Account\n",
    "\n",
    "### Summary\n",
    "\n",
    "This video provides an overview of the OpenAI API playground interface and stresses the critical importance of setting up a billing account with a payment method and configuring usage limits to use the latest models and create API keys, even for testing.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ® Introduces the OpenAI API playground ([platform.openai.com/playground](https://platform.openai.com/playground)) as an interface to test the newest models.\n",
    "- ğŸ”‘ Emphasizes that a billing account (adding a credit card) is required to access the latest models and create API keys, which is a step often missed.\n",
    "- ğŸ’³ Explains how to set up billing by navigating to Settings > Billing > Payment Methods and adding a credit card.\n",
    "- ğŸ”’ Strongly advises setting usage limits and email notifications in the billing settings (Usage Limits) to control spending and prevent unauthorized use.\n",
    "- ğŸ“Š Briefly tours the playground interface sections including Help Center, Forum, API keys, Usage, Storage, Batches, Fine-tuning, Assistants, Completions, and the main Chat interface.\n",
    "- â–¶ï¸ Mentions that the next video will demonstrate how to actively use and \"mess around\" in the OpenAI playground.\n",
    "\n",
    "# The OpenAI Play grounding action\n",
    "\n",
    "### Summary\n",
    "\n",
    "This video explores the functionalities of the OpenAI API playground, demonstrating its three main interfaces (Chat, Assistants, Completions), explaining various model settings for fine-tuned control, showcasing features like viewing generated code and using the Code Interpreter within the Assistants API, and reinforcing the pay-per-token cost model accessible after setting up a billing account.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ•¹ï¸ The OpenAI API playground offers three main modes for interacting with models: Chat, Assistants, and Completions.\n",
    "- ğŸ”§ The Chat interface provides advanced settings like Temperature, Maximum Tokens, Stop Sequences, Top P, Frequency Penalty, and Presence Penalty for detailed control over model output.\n",
    "- ğŸ’» Users can view the underlying code (e.g., Python, Node.js) for their API requests made in the playground, facilitating the transition to building custom applications.\n",
    "- ğŸ¤– The Assistants interface allows creating custom assistants with specific instructions, models, file search, and function calling capabilities, including a demonstration of the Code Interpreter to generate a chart.\n",
    "- ğŸ”’ Data entered in the API playground (and API requests) is explicitly stated as *not* being used to train OpenAI models, ensuring user privacy.\n",
    "- ğŸ’² Using the playground operates on a pay-per-token basis, which can be more cost-effective than a subscription, provided a billing account is set up.\n",
    "- â–¶ï¸ The next video will introduce other APIs to provide a broader understanding before starting application development.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "# Example Python code structure shown in the \"View Code\" feature\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\", # Example model\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and a writing pro.\"}, # System prompt\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a story about a dog that walks a human, a funny story.\"} # User prompt\n",
    "  ],\n",
    "  # Other parameters like temperature, max_tokens, etc. would be included here\n",
    ")\n",
    "# print(response.choices[0].message.content) # Example of how to get the response\n",
    "```\n",
    "\n",
    "# The Google Gemini API: Video Analysis and Other Features\n",
    "\n",
    "### Summary\n",
    "\n",
    "This video provides an overview of the Google Gemini API, accessible through Google Cloud's Vertex AI, discussing its documentation, supported languages and environments, available models, pricing structure compared to OpenAI, and demonstrating its multimodal capabilities in a playground-like interface, although the speaker expresses a preference for OpenAI's models for core development tasks.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸš€ Introduces the Google Gemini API as Google's alternative to OpenAI, with documentation and SDKs available for multiple languages (Python, Android, Node.js, Swift, etc.) and development environments (Google Colab, VS Code).\n",
    "- ğŸ’» Google Cloud's Vertex AI platform is the primary interface for accessing and experimenting with Gemini models like Gemini 1.5 Pro and Gemini Flash.\n",
    "- ğŸ’° Discusses Gemini's pricing, which is token-based and frequently updated, noting it can be competitive or even cheaper than OpenAI for shorter prompts but potentially more expensive for longer ones.\n",
    "- ğŸ§ª Vertex AI offers a playground-like interface to test Gemini models, including multimodal inputs such as uploading and analyzing video content, though reliability issues were observed during demonstration.\n",
    "- ğŸ¤” The speaker expresses a personal preference for OpenAI models due to perceived better behavior and reliability, despite recent competitive changes in Gemini's pricing.\n",
    "- ğŸŒ The primary use case for Google's API for the speaker is integrating Google Search into applications rather than extensive use of the core Gemini models.\n",
    "- ğŸ‘‹ The next video will cover the Anthropic API, continuing the overview of different available APIs.\n",
    "\n",
    "# The Anthropic API for the Claude Models\n",
    "\n",
    "### Summary\n",
    "\n",
    "This video provides an overview of the Anthropic API (for Claude models), comparing its models (Haiku, Sonnet, Opus) and pricing directly with OpenAI and Gemini, concluding that Anthropic's offerings are currently less competitive in terms of cost and features despite having a large context window, leading to the decision not to use it for development in this context.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ¤– Introduces the Anthropic API, offering models named Haiku, Sonnet, and Opus, with Opus being the most capable but also the most expensive.\n",
    "- ğŸ’¸ Provides a direct pricing comparison, stating that Claude Opus is significantly more expensive per token (15x input, 75x output per million tokens) than OpenAI's GPT-4 (3x input, 5x output cheaper), which is perceived as more powerful.\n",
    "- âš–ï¸ Compares Sonnet's price to the best GPT model and Haiku's price to GPT 3.5, finding them generally less competitive.\n",
    "- ğŸ“ Mentions the large 200,000-token context window of Anthropic models but still deems them not competitive at this time.\n",
    "- ğŸ“š Standard API practices like accessing documentation and creating API keys are noted as available.\n",
    "- ğŸ¤” Concludes that currently, the Anthropic API is not competitive with Gemini (which offers features like Search API) and especially not with OpenAI (which has stronger models, Assistant API, DTS), leading to the decision not to use it for development for now.\n",
    "\n",
    "# Summary of the Closed-Source APIs\n",
    "\n",
    "### Summary\n",
    "\n",
    "This video serves as a recap of the previously covered closed-source APIs (OpenAI, Gemini, and Anthropic), comparing their models, pricing, and features, strongly recommending the OpenAI API for application development due to its perceived superior models and competitive pricing, and introduces the next section which will focus on open-source models.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ğŸ”„ Recaps the exploration of the three major closed-source APIs: OpenAI, Gemini (Google), and Anthropic (Claude).\n",
    "- ğŸ”‘ Reemphasizes the necessity of setting up a billing account to use API playgrounds and generate API keys for development, as nothing is permanently free.\n",
    "- ğŸ¥‡ Evaluates the APIs, concluding that OpenAI currently offers the best models with competitive pricing and robust features like the Assistant API and DTS.\n",
    "- â†”ï¸ Positions Gemini's models as acceptable with interesting features like video upload but ultimately less preferred than OpenAI's for core tasks, suggesting experimentation with free credits.\n",
    "- ğŸ“‰ Assesses Anthropic's Claude API as currently the least competitive due to higher costs and models not yet matching the quality of competitors.\n",
    "- ğŸ”§ Highlights that all these APIs allow integrating models into custom applications using API keys, which will be covered later.\n",
    "- ğŸ‘ Recommends adopting the behavior of using the OpenAI API for development projects due to its current standing as the best option.\n",
    "- ğŸ§± Introduces the next section which will shift focus to open-source models, highlighting their potential for local and unrestricted use."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
